 
<!DOCTYPE html>
<html xmlns="//www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">
    <head>
        

        
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-115075361-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-115075361-1');
</script>
        
        <meta http-equiv="content-type" content="text/html; charset=utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1" />

        <title>Standard Test Statistics for OLS Models in R &middot; r-econometrics</title>
        <link rel='stylesheet' href='//fonts.googleapis.com/css?family=Open+Sans:400,300,600' type='text/css'>
        <link rel="stylesheet" href="https://www.r-econometrics.com//libraries/normalize.3.0.1.css" />
        <link rel="stylesheet" href="https://www.r-econometrics.com//css/liquorice.css" />
        <link rel="shortcut icon" href="/favicon.ico" />
        <link rel="apple-touch-icon-precomposed" href="/apple-touch-icon-144-precomposed.png" sizes="144x144" />
        </head>
    <body class="li-body">

<header class="li-page-header">
    <div class="container">
        <div class="row">
            <div class="sixteen columns">
                <div class="li-brand li-left">
                <a href="https://www.r-econometrics.com/">r-econometrics</a></div>
                <div class="li-menu li-right">
                    <span class="li-menu-icon" onclick="javascript:toggle('menu');">&#9776;</span>
                    <ul id="menu2" class="li-menu-items">
                        
                            <li><a href="/rbasicsintro"> R(eal) Basics </a></li>
                        
                            <li><a href="/methodsintro"> Econometric Methods </a></li>
                        
                            <li><a href="/timeseriesintro"> Time Series Topics </a></li>
                        
                            <li><a href="/networksintro"> Network Analysis </a></li>
                        
                            <li><a href="/links"> Links </a></li>
                        
                            <li><a href="/post"> Blog </a></li>
                        
                            <li><a href="/reproductionintro"> Reproduction Projects </a></li>
                        
                            <li><a href="/about"> About and Disclaimer </a></li>
                        
                    </ul>
                </div>
            </div>
        </div>
        <div class="row">
            <div class="sixteen columns">
                <ul id="menu" class="li-menu-items li-menu-mobile">
                    
                        <li><a href="/rbasicsintro"> R(eal) Basics </a></li>
                    
                        <li><a href="/methodsintro"> Econometric Methods </a></li>
                    
                        <li><a href="/timeseriesintro"> Time Series Topics </a></li>
                    
                        <li><a href="/networksintro"> Network Analysis </a></li>
                    
                        <li><a href="/links"> Links </a></li>
                    
                        <li><a href="/post"> Blog </a></li>
                    
                        <li><a href="/reproductionintro"> Reproduction Projects </a></li>
                    
                        <li><a href="/about"> About and Disclaimer </a></li>
                    
                </ul>
            </div>
        </div>
    </div>
</header>






  <meta
     name="description"
     content="An overview of how to calculate standard test statistics in R, which should be applied to any econometric analysis that is based on OLS."
   />

    <div class="container">
        <div class="row">
            <div class="sixteen columns">
                <article class="li-article">
                    <header class="li-article-header">
                        <h1 class="li-article-title">Standard Test Statistics for OLS Models in R</h1>
                        <span class="li-article-taxonomies">
                            

                            
                                with tags
                                
                                    <a href="https://www.r-econometrics.com//tags/normality-test">normality-test</a>
                                
                                    <a href="https://www.r-econometrics.com//tags/t-test">t-test</a>
                                
                                    <a href="https://www.r-econometrics.com//tags/f-test">F-test</a>
                                
                                    <a href="https://www.r-econometrics.com//tags/hausman-test">hausman-test</a>
                                
                            
                        </span>
                         - 
                        <time class="li-article-date">
                          Franz X. Mohr, Created: November 25, 2019, Last update: November 25, 2019
                         
                        </time>
                    </header>
                    <section>
                        


<p>Model testing belongs to the main tasks of any econometric analysis. This post gives an overview of tests, which should be applied to OLS regressions, and illustrates how to calculate them in R. The focus of the post is rather on the calcuation of the tests. For a treatment of mathematical details, please, consult a standard textbook. This list of statistical tests is necessarily incomplete. However, if you have a strong opinion that a specific test is missing, feel free to leave a comment below.</p>
<div id="data-and-model" class="section level2">
<h2>Data and model</h2>
<p>To illustrate the calculation of test statistics in R, let’s use the <em>wage1</em> dataset from the <code>wooldridge</code> package and estimate a basic <a href="https://en.wikipedia.org/wiki/Mincer_earnings_function" target="_blank">Mincer earnings function</a>. This standard specification of earnings models explains the natural log of average hourly earnings <code>lwage</code> by years of education <code>educ</code> and experience <code>exper</code>. The standard specification also includes squared values of experience <code>expersq</code> to take into account potential decreasing marginal effects.</p>
<pre class="r"><code># Load dataset
library(wooldridge)
data(&quot;wage1&quot;)

# Estimate a model
model &lt;- lm(lwage ~ educ + exper + expersq, data = wage1)</code></pre>
</div>
<div id="significance-of-variables" class="section level2">
<h2>Significance of variables</h2>
<div id="t-test" class="section level3">
<h3>t test</h3>
<p>t tests are used to assess the statistical significance of single variables. In R t values for each variable in a regression model are usually calculated by the <code>summary</code> function:</p>
<pre class="r"><code>summary(model)</code></pre>
<pre><code>## 
## Call:
## lm(formula = lwage ~ educ + exper + expersq, data = wage1)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -1.96387 -0.29375 -0.04009  0.29497  1.30216 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  0.1279975  0.1059323   1.208    0.227    
## educ         0.0903658  0.0074680  12.100  &lt; 2e-16 ***
## exper        0.0410089  0.0051965   7.892 1.77e-14 ***
## expersq     -0.0007136  0.0001158  -6.164 1.42e-09 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.4459 on 522 degrees of freedom
## Multiple R-squared:  0.3003, Adjusted R-squared:  0.2963 
## F-statistic: 74.67 on 3 and 522 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>The t values for our benchmark model indicate that, except for the constant, all variables are statistically significant. You might think about dropping the intercept term at this point, but let’s forget about this for the moment.</p>
</div>
<div id="f-test" class="section level3">
<h3>F test</h3>
<p>F tests can be used to check, whether one or multiple variables in a model are statistically significant. Basically, the test compares two models with each other, where one model is a special case of the other. This means that we compare a model with more variables - the so-called <em>unrestricted</em> model - to a model with less but otherwise the same variables, i.e. the <em>restriced</em> or nested model. If the additional predictive power of the unrestricted model is sufficiently higher, the variables are jointly significant.</p>
<p>In our example we add the <code>tenure</code> variable and its square <code>tenuresq</code> to the model equation. This model is now the unrestricted model, which we have to estimate before we can calculate the F test.</p>
<pre class="r"><code># Estimate unrestricted model
model_unres &lt;- lm(lwage ~ educ + exper + expersq + tenure + tenursq, data = wage1)

summary(model_unres)</code></pre>
<pre><code>## 
## Call:
## lm(formula = lwage ~ educ + exper + expersq + tenure + tenursq, 
##     data = wage1)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -1.96984 -0.25313 -0.03204  0.27141  1.28302 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  0.2015715  0.1014697   1.987   0.0475 *  
## educ         0.0845258  0.0071614  11.803  &lt; 2e-16 ***
## exper        0.0293010  0.0052885   5.540 4.80e-08 ***
## expersq     -0.0005918  0.0001141  -5.189 3.04e-07 ***
## tenure       0.0371222  0.0072432   5.125 4.20e-07 ***
## tenursq     -0.0006156  0.0002495  -2.468   0.0139 *  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.425 on 520 degrees of freedom
## Multiple R-squared:  0.3669, Adjusted R-squared:  0.3608 
## F-statistic: 60.26 on 5 and 520 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>The t values of the new model indicate that all variabes - including the constant - are individually, statistically significant. Thus, the variable <code>tenure</code> provides additional explanatory power to the model. Since <code>tenure</code> also enters in its squared form, we are interested in the joint significance of the tenure terms. This can be checked with an F test, which can be calculated by using the <code>anova</code> function in R. We only have to provide the two estimated models as arguments:</p>
<pre class="r"><code>anova(model, model_unres)</code></pre>
<pre><code>## Analysis of Variance Table
## 
## Model 1: lwage ~ educ + exper + expersq
## Model 2: lwage ~ educ + exper + expersq + tenure + tenursq
##   Res.Df     RSS Df Sum of Sq      F    Pr(&gt;F)    
## 1    522 103.790                                  
## 2    520  93.911  2    9.8791 27.351 5.079e-12 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>As you can see from the output, the tenure terms are joinly significant as indicated by the low value in column <code>Pr(&gt;F)</code>.</p>
<p><em>Note that both the <span class="math inline">\(t\)</span> test and the <span class="math inline">\(F\)</span> test are only valid in situations, where the relation between the dependent and independent variables are linear. For non-linear cases the likelihood ratio (LR) test, the Wald (W) test, and the Lagrange multiplier (LM) test can be used.</em></p>
</div>
</div>
<div id="normality-of-the-errors" class="section level2">
<h2>Normality of the errors</h2>
<p>One key assumption of many test statistics is that the errors of a model are normally distributed. If this is not the case, some of the obtained results might become questionable. There is a series of normality tests, which are also listed on <a href="https://en.wikipedia.org/wiki/Normality_test" target="_blank">Wikipedia: Normality tests</a>. A selection of these tests is presented in the following.</p>
<p><em>Note that Kennedy (2014) mentions these tests in the context of outlier detection. This is intuitive, since outliers usually lie outside of the assumed (normal) distribution.</em></p>
<div id="tests" class="section level3">
<h3>Tests</h3>
<ul>
<li>Eyeball test: A rather intuitive way to test for normality is to simply look at the histogram of the model residuals:</li>
</ul>
<pre class="r"><code>hist(model$residuals, breaks = 20)</code></pre>
<p><img src="/methods/teststats_files/figure-html/unnamed-chunk-2-1.png" width="432" /></p>
<p>Since the shape of the histogram resembles the famous bell shape of a normal distribution, this looks pretty normal to me. However, there are more formal ways to test for normality, which are required by a more sophisticated audience than me…</p>
<ul>
<li>The <a href="https://en.wikipedia.org/wiki/Shapiro%E2%80%93Wilk_test" target="_blank">Shapiro-Wilk test</a> is considered to be a very reliable test for normality. It is implemented in the <code>shapiro.test</code> function of the <code>stats</code> package:</li>
</ul>
<pre class="r"><code>shapiro.test(model$residuals)</code></pre>
<pre><code>## 
##  Shapiro-Wilk normality test
## 
## data:  model$residuals
## W = 0.99258, p-value = 0.01031</code></pre>
<p>The low p-value indicates that the null hypothesis of a normal distribution is not rejected at the 1 percent level.</p>
<ul>
<li>The <a href="https://en.wikipedia.org/wiki/Anderson%E2%80%93Darling_test" target="_blank">Anderson–Darling test</a> is also a popular normality test. It also has the advantage that it can be applied to other distributions as well. To test for normal distributions in R, the <code>ad.test</code> function of the <code>nortest</code> package can be used:</li>
</ul>
<pre class="r"><code>library(nortest)

ad.test(model$residuals)</code></pre>
<pre><code>## 
##  Anderson-Darling normality test
## 
## data:  model$residuals
## A = 0.86991, p-value = 0.02559</code></pre>
<p>The low p-value also indicates that the null hypothesis of a normal distribution is not rejected at the 2.56 percent level.</p>
<ul>
<li>The <a href="https://en.wikipedia.org/wiki/Jarque%E2%80%93Bera_test" target="_blank">Jarque-Bera test</a> tests whether the skewness and kurtosis of a model’s residuals match the expected values for normal distribution. In R the function <code>jarque.bera.test</code> of the <code>tseries</code> packages and the function <code>jarque.test</code> in the <code>moments</code> package are, among others, implementations of this test:</li>
</ul>
<pre class="r"><code>library(tseries)

jarque.bera.test(model$residuals)</code></pre>
<pre><code>## 
##  Jarque Bera Test
## 
## data:  model$residuals
## X-squared = 7.1519, df = 2, p-value = 0.02799</code></pre>
<p>Again, the low p-value of the Jarque-Bera test indicates that the skewness and kurtosis of the test are not significantly different from their expected values for normal distribution. Thus, the distribution of the model residuals seems statistically not different from a normal distribution.</p>
</div>
<div id="antidotes" class="section level3">
<h3>Antidotes</h3>
<p>In order to deal with non-normality in the errors, you could consider the following options:</p>
<ul>
<li>Use the log of the dependent variable for you regression.</li>
<li>For time series data use the first difference of the dependent variable, because you might have a stationarity issue.</li>
<li>Use a different estimator. For example, if your dependet varible can only have positive values, you might be better off with a generalised linear model, which assumes Gamma-distributed errors. Or for count data you might want to use a Poisson estimator etc.</li>
</ul>
</div>
</div>
<div id="heteroskedasticity" class="section level2">
<h2>Heteroskedasticity</h2>
<p>Heteroskedasticity arises when the variance of an observation’s error depends on the size of the regressors. Although this does not lead to a bias of the estimated coefficients, it can still render test statistics useless, because their validity relies on the assumption of so-called <em>homoskedastic</em> errors, i.e. the assumption that the errors do not correlate with explanatory variables.</p>
<div id="tests-1" class="section level3">
<h3>Tests</h3>
<ul>
<li>Eyeball test: One way to check for heteroskedasticity is to look at a plot, where the absolute values of the residuals - or the squared residuals - are plotted against the regressors of the model. If the size of the absolute (or squared) errors appears to vary with the size of a regressor, this migth indicate heteroskedasticity.</li>
</ul>
<pre class="r"><code># Create plot input
hetsk_data &lt;- cbind(abs(model$residuals), model$model[, -1])

# Scatterplot matrix
plot(hetsk_data)</code></pre>
<p><img src="/methods/teststats_files/figure-html/heterosk_eyeball-1.png" width="528" /></p>
<p>By looking at the first row of the above scatter plot we might notice that the absolute errors seem to be larger for higher values of <code>educ</code> and lower for higher values of <code>expersq</code>. This might warrant to test this more formally.</p>
<ul>
<li>The <a href="https://en.wikipedia.org/wiki/Breusch%E2%80%93Pagan_test" target="_blank">Breusch-Pagan test</a> is implemented in the function <code>pbtest</code> both of the <code>AER</code> and <code>lmtest</code> package and tests whether the variance of a model’s residuals dependents on the values of the regressors.</li>
</ul>
<pre class="r"><code>library(AER)

bptest(model)</code></pre>
<pre><code>## 
##  studentized Breusch-Pagan test
## 
## data:  model
## BP = 19.84, df = 3, p-value = 0.0001832</code></pre>
<ul>
<li>The <a href="https://en.wikipedia.org/wiki/White_test" target="_blank">White test</a> regresses the squared residuals from the original regression model onto a set of regressors that contain the original regressors along with their squares and cross-products. Since it builds on the Breusch-Pagan test, the <code>bptest</code> can be used to perform the White test by adding the dependend variables and its squared values to the estimation formula:</li>
</ul>
<pre class="r"><code>bptest(model, ~ lwage + I(lwage^2), data = wage1)</code></pre>
<pre><code>## 
##  studentized Breusch-Pagan test
## 
## data:  model
## BP = 319.9, df = 2, p-value &lt; 2.2e-16</code></pre>
<ul>
<li>The <a href="https://en.wikipedia.org/wiki/Goldfeld%E2%80%93Quandt_test" target="_blank">Goldfeld-Quandt test</a> basically devides the sample into two groups with low and high values of an explanatory variable and compares the variances of two submodels. If the variances differ, this indicates heteroskedasticity. It is implemented in the function <code>gqtest</code> of the <code>lmtest</code> package:</li>
</ul>
<pre class="r"><code>library(lmtest)

gqtest(model)</code></pre>
<pre><code>## 
##  Goldfeld-Quandt test
## 
## data:  model
## GQ = 0.74514, df1 = 259, df2 = 259, p-value = 0.9909
## alternative hypothesis: variance increases from segment 1 to 2</code></pre>
<p><em>Note that this test has a series of drawbacks, which is why it is not used very often. The Breusch-Pagan and White test are used much more frequently.</em></p>
</div>
<div id="antidotes-1" class="section level3">
<h3>Antidotes</h3>
<ul>
<li>If the basic structure of heteroskedasticity is unknown - as is usually the case - use <a href="/methods/hcrobusterrors">heteroskedasticity robust standard errors</a>.</li>
<li>If the researcher feels that the structure of heteroskedasticity can be estimated, use the <a href="https://en.wikipedia.org/wiki/Generalized_least_squares#Feasible_generalized_least_squares" target="_blank">feasible GLS</a> estimator</li>
</ul>
</div>
</div>
<div id="correlation-between-explanatory-variables-and-the-error-term" class="section level2">
<h2>Correlation between explanatory variables and the error term</h2>
<p>If explanatory variables are correlated with the error term, i.e. when they are <em>endogenous</em>, an OLS estimator will produce biased estimates. Even if only one variable is causing this problem, the coefficients of all other variables will be affected as well. Since economic data is frequently affected by this problem - due to, for example, measurement error in explanatory variables, autoregression with autocorrelated errors, simultaneity, omitted explanatory variables or sample selection - it is important to test for it.</p>
<div id="hausman-test" class="section level3">
<h3>Hausman test</h3>
<p>A popular method to test for correlation between explanatory variables and the error term is the <em>Hausman test</em>. It basically tests whether the results of the IV estimator are significantly different from the OLS estimator. To illustrate this, let’s use the <em>wage2</em> dataset from the <code>wooldridge</code> package and, again, estimate a basic <a href="https://en.wikipedia.org/wiki/Mincer_earnings_function" target="_blank">Mincer earnings function</a> as described above. However, this time we add <code>IQ</code> as a measure of <em>ability</em>. But since it is assumed that the measure is correlated with the error term, we use an additional measure of ability, <code>KWW</code>, as an instrument for <code>IQ</code>.</p>
<pre class="r"><code>library(dplyr)
library(wooldridge)
data(&quot;wage2&quot;)

# Add squared experience &quot;expersq&quot;
wage2 &lt;- wage2 %&gt;%
  mutate(expersq = exper^2)

# IV estimator
iv_model &lt;- ivreg(lwage ~ educ + exper + expersq + IQ | KWW + educ + exper + expersq,
                  data = wage2)</code></pre>
<p>The <code>summary</code> function of the <code>ivreg</code> object, performs the Hausman test automatically. We only have to add the argument <code>diagnostics = TRUE</code>.</p>
<pre class="r"><code>summary(iv_model, diagnostics = TRUE)</code></pre>
<pre><code>## 
## Call:
## ivreg(formula = lwage ~ educ + exper + expersq + IQ | KWW + educ + 
##     exper + expersq, data = wage2)
## 
## Residuals:
##       Min        1Q    Median        3Q       Max 
## -2.097500 -0.256972  0.005075  0.274838  1.284012 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 4.5137035  0.2437555  18.517  &lt; 2e-16 ***
## educ        0.0096496  0.0155761   0.620    0.536    
## exper       0.0144617  0.0145717   0.992    0.321    
## expersq     0.0001935  0.0006100   0.317    0.751    
## IQ          0.0191398  0.0038791   4.934 9.54e-07 ***
## 
## Diagnostic tests:
##                  df1 df2 statistic  p-value    
## Weak instruments   1 930     77.13  &lt; 2e-16 ***
## Wu-Hausman         1 929     15.63 8.26e-05 ***
## Sargan             0  NA        NA       NA    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.4231 on 930 degrees of freedom
## Multiple R-Squared: -0.004854,   Adjusted R-squared: -0.009176 
## Wald test: 36.38 on 4 and 930 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>The (Wu-)Hausman test statistic has a very low p-value and, hence, the test rejects the null of no correcation between <code>IQ</code> and the error term in this example.</p>
</div>
<div id="sargan-test" class="section level3">
<h3>Sargan test</h3>
<p>The Sargan test checks whether an instrument is correlated with the errors, which should not be the case for good instruments. In the example above, the test statistic of the Sargan test is <code>NA</code>. This is to be expected, because we use <em>one</em> instrument for <em>one</em> endogenous regressor and the mathematical properties of the test require us to have at least one instrument more than endogenous regressor. So, if we use more instruments than endogenous regressors, the system would be identified and we would be able to obtain a test statistic. A low statistic (with rather high p-value) would indicate that the instrument is uncorrelated with the error term.</p>
<p><em>To be continued… (January 2020)</em></p>
<!-- ## Autocorrelated errors -->
<!-- Similar to the problem of heteroskedasticity the presence of autocorrelated errors do not lead to a bias of the estimated coefficients, but they can render test statistics useless. This problem is most relevant for time series data. -->
<!-- ### Tests -->
<!-- Durbin Watson test -->
<!-- ### Antidotes -->
<!-- * HEC estimates -->
<!-- * Cochrane-Orcut and Prais-Winsten estimator: -->
<!-- * Durbin's two-stage method -->
<!-- * Hildreth-Lu Search Procedure -->
<!-- * Maximum Likelihood: Instead of a simle OLS estimator, we could use the maximum likelihood estimator, which requires to make a bit stronger assumption about the distribution of the errors. -->
<!-- * Bayesian estimator: That would lead too far. -->
<!-- ## Multicollinearity -->
<!-- ### Tests -->
<!-- Inflated R-squared -->
<!-- ## Outliers -->
<!-- Can seriously alter your estimation results -->
<!-- ### Tests -->
<!-- * Box-plot -->
<!-- * Cook outlier test -->
<!-- ### Antidotes -->
</div>
</div>
<div id="literature" class="section level2">
<h2>Literature</h2>
<p>Kennedy, P. (2014). <em>A Guide to Econometrics</em>. Malden (Mass.): Blackwell Publishing 6th ed.</p>
<p>Wooldridge, J.M. (2013). <em>Introductory econometrics: A modern approach</em>. (5<sup>th</sup>ed.)</p>
</div>

                    </section>
                </article>
                 <div class="container">
  <div class="row">
    <div class="sixteen columns center">
      <div class="addthis_tipjar_inline"></div>
      <div class="addthis_inline_share_toolbox"></div>
    </div>
  </div>
</div>

                 
<script type="text/javascript" src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5a9c6c9b8a39890d"></script>
                
                <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

<footer class="li-page-footer">
    <div class="container">
        <div class="row">
            <div class="sixteen columns center">
              
              <div class="commentbox"></div>
<script src="https://unpkg.com/commentbox.io/dist/commentBox.min.js"></script>
<script>commentBox('5679369011855360-proj')</script>
              
                  
                  
                  
                  
            </div>
        </div>
    </div>
</footer>
            </div>
        </div>
        
    </div>
<footer class="li-page-footer">
    <div class="container">
        <div class="row">
            <div class="sixteen columns center">
              <div class="li-page-footer-legal">
                &copy; Franz X. Mohr. Powered by <a href="https://github.com/rstudio/blogdown" target="_blank" >Blogdown</a> and hosted by <a href="https://www.netlify.com/" target="_blank" >Netlify</a>. Source available on <a href="https://github.com/franzmohr/r-econometrics" target="_blank" >Github</a>.
              </div>
              <div class="li-page-footer-theme">
                <span class=""><a href="https://github.com/eliasson/liquorice/">liquorice</a> is a theme for <a href="https://gohugo.io/">hugo</a>
                </span>
              </div>
            </div>
        </div>
    </div>
</footer>

    <script src="//yihui.name/js/math-code.js"></script>
    <script async
      src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>

    <script type="text/javascript">
    
    function toggle(id) {
        var e = document.getElementById(id);
        e.style.display == 'block' ? e.style.display = 'none' : e.style.display = 'block';
    }

    </script>
    
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-115075361-1', 'auto');
	
	ga('send', 'pageview');
}
</script>

    </body>
</html>

